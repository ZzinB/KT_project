{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#**Aivle 스쿨 지원 질문, 답변 챗봇 만들기**\n","# 단계2 : 모델링"],"metadata":{"id":"3xIPZjFU5rjt"}},{"cell_type":"markdown","source":["## 0.미션"],"metadata":{"id":"-FPypzell2uc"}},{"cell_type":"markdown","source":["* 다음 세가지 챗봇을 만들고 비교해 봅시다.\n","* 챗봇1. Word2Vec 임베딩 벡터 기반 머신러닝 분류 모델링\n","    * Word2Vec 모델을 만들고 임베딩 벡터를 생성합니다.\n","    * 임베딩 벡터를 이용하여 intent를 분류하는 모델링을 수행합니다.\n","        * 이때, LightGBM을 추천하지만, 다른 알고리즘을 이용할수 있습니다.\n","    * 예측된 intent의 답변 중 임의의 하나를 선정하여 출력합니다.\n","* 챗봇2. 단계별 모델링1\n","    * 1단계 : type(일상대화 0, 에이블스쿨Q&A 1) 분류 모델 만들기\n","        * Embedding + LSTM 모델링\n","    * 2단계 : 사전학습된 Word2Vec 모델을 로딩하여 train의 임베딩벡터 저장\n","    * 코사인 유사도로 intent 찾아 답변 출력\n","        * 새로운 문장의 임베딩벡터와 train의 임베딩 벡터간의 코사인 유사도 계산\n","        * 가장 유사도가 높은 질문의 intent를 찾아 답변 출력하기\n","* 챗봇3. 단계별 모델링2\n","    * 1단계 : 챗봇2의 1단계 모델을 그대로 활용\n","    * 2단계 : FastText 모델 생성하여 train의 임베딩벡터 저장\n","    * 코사인 유사도로 intent 찾아 답변 출력\n","        * 새로운 문장의 임베딩벡터와 train의 임베딩 벡터간의 코사인 유사도 계산\n","        * 가장 유사도가 높은 질문의 intent를 찾아 답변 출력하기\n","\n","* 챗봇3개에 대해서 몇가지 질문을 입력하고 각각의 답변을 비교해 봅시다.\n"],"metadata":{"id":"AC6wpFtQtR5y"}},{"cell_type":"markdown","source":["## 1.환경준비"],"metadata":{"id":"pvBAaxSgrkt7"}},{"cell_type":"markdown","source":["### (1)라이브러리 설치"],"metadata":{"id":"AvBsTv3s-a3X"}},{"cell_type":"markdown","source":["#### 1) gensim 3.8.3 설치"],"metadata":{"id":"EdhloaEO-mFZ"}},{"cell_type":"code","source":["#gensim은 자연어 처리를 위한 오픈소스 라이브러리입니다. 토픽 모델링, 워드 임베딩 등 다양한 자연어 처리 기능을 제공\n","# 현재 4.x 버전이 최신이지만, 3.8.3 버전으로 진행\n","!pip uninstall gensim\n","!pip install gensim==3.8.3"],"metadata":{"id":"Pkk8tlp2-Q3g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683345780783,"user_tz":-540,"elapsed":70568,"user":{"displayName":"이신비","userId":"04527371589362270556"}},"outputId":"4fbf4636-8a46-49fa-8486-86c78de99546"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: gensim 4.3.1\n","Uninstalling gensim-4.3.1:\n","  Would remove:\n","    /usr/local/lib/python3.10/dist-packages/gensim-4.3.1.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/gensim/*\n","Proceed (Y/n)? Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\n","    status = run_func(*args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/uninstall.py\", line 105, in run\n","    uninstall_pathset = req.uninstall(\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 664, in uninstall\n","    uninstalled_pathset.remove(auto_confirm, verbose)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 367, in remove\n","    if auto_confirm or self._allowed_to_proceed(verbose):\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 407, in _allowed_to_proceed\n","    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 191, in ask\n","    response = input(message)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n","    return self._cache[level]\n","KeyError: 50\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/pip3\", line 8, in <module>\n","    sys.exit(main())\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n","    return command.main(cmd_args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n","    return self._main(args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 214, in _main\n","    return run(options, args)\n","  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 197, in exc_logging_wrapper\n","    logger.critical(\"Operation cancelled by user\")\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n","    if self.isEnabledFor(CRITICAL):\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 1734, in isEnabledFor\n","    _acquireLock()\n","  File \"/usr/lib/python3.10/logging/__init__.py\", line 226, in _acquireLock\n","    _lock.acquire()\n","KeyboardInterrupt\n","^C\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gensim==3.8.3\n","  Using cached gensim-3.8.3.tar.gz (23.4 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.22.4)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.10.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (1.16.0)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.8.3) (6.3.0)\n","Building wheels for collected packages: gensim\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n","Failed to build gensim\n","Installing collected packages: gensim\n","  Attempting uninstall: gensim\n","    Found existing installation: gensim 4.3.1\n","    Uninstalling gensim-4.3.1:\n","      Successfully uninstalled gensim-4.3.1\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mRunning setup.py install for gensim\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Running setup.py install for gensim ... \u001b[?25l\u001b[?25herror\n","  Rolling back uninstall of gensim\n","  Moving to /usr/local/lib/python3.10/dist-packages/gensim-4.3.1.dist-info/\n","   from /usr/local/lib/python3.10/dist-packages/~ensim-4.3.1.dist-info\n","  Moving to /usr/local/lib/python3.10/dist-packages/gensim/\n","   from /usr/local/lib/python3.10/dist-packages/~ensim\n","\u001b[1;31merror\u001b[0m: \u001b[1mlegacy-install-failure\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while trying to install package.\n","\u001b[31m╰─>\u001b[0m gensim\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for output from the failure.\n"]}]},{"cell_type":"markdown","source":["* [코랩] 위 라이브러리 설치후 런타임 재시작 필요!"],"metadata":{"id":"Yvo0_3so-dA8"}},{"cell_type":"markdown","source":["#### 2) 형태소 분석을 위한 라이브러리"],"metadata":{"id":"w9I84wAV-EQ7"}},{"cell_type":"code","source":["# mecab 설치를 위한 관련 패키지 설치\n","!apt-get install curl git\n","!apt-get install build-essential\n","!apt-get install cmake\n","!apt-get install g++\n","!apt-get install flex\n","!apt-get install bison\n","!apt-get install python-dev\n","!pip install cython\n","!pip install mecab-python"],"metadata":{"id":"eRgLSScn0QFG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683345814117,"user_tz":-540,"elapsed":18506,"user":{"displayName":"이신비","userId":"04527371589362270556"}},"outputId":"26ee4500-c503-4040-a060-c44747b57a6f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","curl is already the newest version (7.68.0-1ubuntu2.18).\n","git is already the newest version (1:2.25.1-1ubuntu3.11).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","build-essential is already the newest version (12.8ubuntu1.1).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","cmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","g++ is already the newest version (4:9.3.0-1ubuntu2).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","flex is already the newest version (2.6.4-6.2).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","bison is already the newest version (2:3.5.1+dfsg-1).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Note, selecting 'python-dev-is-python2' instead of 'python-dev'\n","python-dev-is-python2 is already the newest version (2.7.17-4).\n","0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (0.29.34)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mecab-python in /usr/local/lib/python3.10/dist-packages (1.0.0)\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.10/dist-packages (from mecab-python) (1.0.6)\n"]}]},{"cell_type":"code","source":["# 형태소 기반 토크나이징 (Konlpy)\n","!python3 -m pip install konlpy\n","# mecab (ubuntu: linux, mac os 기준)\n","# 다른 os 설치 방법 및 자세한 내용은 다음 참고: https://konlpy.org/ko/latest/install/#id1\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"],"metadata":{"id":"7zwNEdPo3Rpm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683345822604,"user_tz":-540,"elapsed":3910,"user":{"displayName":"이신비","userId":"04527371589362270556"}},"outputId":"393c830c-0508-4e05-d728-b4bafee8a21a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n","mecab-ko is already installed\n","mecab-ko-dic is already installed\n","mecab-python is already installed\n","Done.\n"]}]},{"cell_type":"markdown","source":["### (2) 라이브러리 불러오기"],"metadata":{"id":"FlRWJB2w6Ip6"}},{"cell_type":"markdown","source":["* 세부 요구사항\n","    - 기본적으로 필요한 라이브러리를 import 하도록 코드가 작성되어 있습니다.\n","    - 필요하다고 판단되는 라이브러리를 추가하세요."],"metadata":{"id":"-TRDCUpP6Ip6"}},{"cell_type":"code","metadata":{"id":"h1IYbPd_6Ip6","executionInfo":{"status":"ok","timestamp":1683345830371,"user_tz":-540,"elapsed":4614,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import joblib\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","# 필요하다고 판단되는 라이브러리를 추가하세요.\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMClassifier\n","from sklearn.metrics import * \n","\n","import tensorflow as tf\n","from keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n","from keras import Input, Model\n","from keras import optimizers\n","from keras.models import Sequential, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["* 형태소 분석을 위한 함수를 제공합니다."],"metadata":{"id":"ERab2qbnVloB"}},{"cell_type":"code","source":["from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n","\n","# 다양한 토크나이저를 사용할 수 있는 함수\n","def get_tokenizer(tokenizer_name):\n","    if tokenizer_name == \"komoran\":\n","        tokenizer = Komoran()\n","    elif tokenizer_name == \"okt\":\n","        tokenizer = Okt()\n","    elif tokenizer_name == \"mecab\":\n","        tokenizer = Mecab()\n","    elif tokenizer_name == \"hannanum\":\n","        tokenizer = Hannanum()\n","    else:\n","        # \"kkma\":\n","        tokenizer = Kkma()\n","        \n","    return tokenizer"],"metadata":{"id":"dGr3phdYVloC","executionInfo":{"status":"ok","timestamp":1683345830371,"user_tz":-540,"elapsed":5,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 형태소 분석을 수행하는 함수\n","\n","def tokenize(tokenizer_name, original_sent, nouns=False):\n","    # 미리 정의된 몇 가지 tokenizer 중 하나를 선택\n","    tokenizer = get_tokenizer(tokenizer_name)\n","\n","    # tokenizer를 이용하여 original_sent를 토큰화하여 tokenized_sent에 저장하고, 이를 반환합니다.\n","    sentence = original_sent.replace('\\n', '').strip()\n","    if nouns:       \n","        # tokenizer.nouns(sentence) -> 명사만 추출\n","        tokens = tokenizer.nouns(sentence)\n","    else:\n","        tokens = tokenizer.morphs(sentence)\n","    tokenized_sent = ' '.join(tokens)\n","    \n","    return tokenized_sent"],"metadata":{"id":"f1kGJuX6VloD","executionInfo":{"status":"ok","timestamp":1683345833895,"user_tz":-540,"elapsed":593,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### (3) 데이터 로딩\n","* 전처리 단계에서 생성한 데이터들을 로딩합니다.\n","    * train, test\n","    * 형태소분석 결과 데이터 : clean_train_questions, clean_test_questions"],"metadata":{"id":"wsLDv9tZc_i1"}},{"cell_type":"markdown","source":["* 구글 드라이브 연결"],"metadata":{"id":"Uc_kIeeJeDgi"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"dd0SPbYdfhS9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683345839678,"user_tz":-540,"elapsed":3261,"user":{"displayName":"이신비","userId":"04527371589362270556"}},"outputId":"6315c76d-8c48-4b2c-8bbc-862011b79fa6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/0417_미니프로젝트 6차_실습자료/'"],"metadata":{"id":"y5OIDazoeIN4","executionInfo":{"status":"ok","timestamp":1683345843096,"user_tz":-540,"elapsed":507,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["* 저장된 .pkl 파일들을 불러옵니다.\n","* 불러 온 후에는 shape를 확인해 봅시다."],"metadata":{"id":"GH3ApIzofYPb"}},{"cell_type":"code","source":["train = joblib.load(path + 'train.pkl')\n","test = joblib.load(path + 'test.pkl')\n","clean_test_questions = joblib.load(path + 'clean_test_questions.pkl')\n","clean_train_questions = joblib.load(path + 'clean_train_questions.pkl')"],"metadata":{"id":"vjQCIzsUsKJD","executionInfo":{"status":"ok","timestamp":1683345844610,"user_tz":-540,"elapsed":3,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train.shape, test.shape, clean_test_questions.shape, clean_train_questions.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3B7faTK7YeG","executionInfo":{"status":"ok","timestamp":1683345846526,"user_tz":-540,"elapsed":4,"user":{"displayName":"이신비","userId":"04527371589362270556"}},"outputId":"fdaeb4ec-95b5-4727-88e1-77d12c2640a7"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1113, 4), (106, 4), (106,), (1113,))"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[],"metadata":{"id":"sY4abSWpW4lb","executionInfo":{"status":"ok","timestamp":1683345332506,"user_tz":-540,"elapsed":19,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## 2.챗봇1"],"metadata":{"id":"mMQvj3AmIDEy"}},{"cell_type":"markdown","source":["* **상세요구사항**\n","    * Word2Vec을 활용한 LightGBM 모델링(intent 분류)\n","        * Word2Vec을 이용하여 임베딩벡터 생성하기\n","            * Word Embedding으로 문장벡터 구하기\n","        * 임베딩 벡터를 이용하여 ML기반 모델링 수행하기\n","            * LightGBM 권장(다른 알고리즘을 이용할수 있습니다.)\n","    * 챗봇 : 모델의 예측결과(intent)에 따라 답변하는 챗봇 만들기\n","        * 질문을 입력받아, 답변하는 함수 생성"],"metadata":{"id":"_x9L3XAfJPAh"}},{"cell_type":"markdown","source":["### (1) Word2Vec을 이용하여 임베딩벡터 생성하기\n","* 'mecab' 형태소 분석기를 이용하여 문장을 tokenize\n","    * Word2Vec 모델을 만들기 위해서 입력 데이터는 리스트 형태여야 합니다.\n","    * 그래서 다시 리스트로 저장되도록 토크나이즈 해 봅시다.\n","* Word Embedding으로 문장벡터를 생성합니다.\n","    * 먼저 Word2Vec 모델을 만들고, train의 질문들을 문장벡터로 만듭시다.\n"],"metadata":{"id":"1lQMnaY2SIKM"}},{"cell_type":"markdown","source":["#### 1) 'mecab' 형태소 분석기를 이용하여 문장을 tokenize"],"metadata":{"id":"XZtMR3HVRfCI"}},{"cell_type":"code","source":["#train의 질문들을 리스트로 뽑고\n","raw_sentence = list(train['Q'])\n","\n","# train_sentences 리스트를 초기화\n","train_sentence = []\n","\n","# raw_swmtemce에서 하나씩 가져와서 토큰화하여 train_sentence에 추가한다\n","for sent in raw_sentence:\n","    # tokenize함수를 통해 mecab형태소 분석기를 이용하여 문장을 토큰화한다\n","    # split함수를 이용하여 토큰화된 단어들을 리스트로 만들어 추가한다\n","    train_sentence.append(tokenize('mecab', sent).split(' '))\n","\n","print(train_sentence[:5])"],"metadata":{"id":"FHwKdpLvJPc-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683345850053,"user_tz":-540,"elapsed":1151,"user":{"displayName":"이신비","userId":"04527371589362270556"}},"outputId":"f630ceec-b07f-46bc-a9fb-fb6b28bc637e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[['떨어뜨려서', '핸드폰', '액정', '나갔', '어'], ['핸드폰', '떨어뜨려서', '고장', '났', '나', '봐'], ['노트북', '이', '작동', '이', '안', '되', '네'], ['노트북', '키보드', '가', '안', '먹히', '네'], ['노트북', '전원', '이', '안', '들어와', '.']]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"YLHDaEeKATKL","executionInfo":{"status":"ok","timestamp":1683345333622,"user_tz":-540,"elapsed":8,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["#### 2) Word Embedding으로 문장벡터 구하기\n","* Word2Vec\n","    * 위에서 저장한 입력 데이터를 사용하여 Word2Vec 모델이 생성\n","    * 모델은 size(단어 벡터의 차원), \n","    * window(컨텍스트 창의 크기), \n","    * max_vocab_size(고려할 최대 어휘 크기), \n","    * min_count(포함할 단어의 최소 빈도)와 같은 특정 하이퍼파라미터로 훈련됩니다.\n","    * sg : 사용할 훈련 알고리즘 - 1은 skip-gram, 0은 CBOW )"],"metadata":{"id":"ZsDPBh8Nhxy2"}},{"cell_type":"code","source":["# Word2Vec은 임베딩할 때, 워드 벡터의 차원 수를 지정하는 변수로 100 지정되어 있음\n","SIZE = 100\n","# Word2Vec 모델에서 한 단어의 좌우에 있는 최대 단어 개수를 지정하는 변수로 3 지정되어 있음\n","WINDOW = 3\n","# Word2Vec 모델에 의해 무시될 최소 단어 빈도 수를 지정하는 변수로 해당 값보다 적게 나타나는 단어 수\n","MIN_COUNT = 5"],"metadata":{"id":"V6Y26nTvJ1si","executionInfo":{"status":"ok","timestamp":1683345853920,"user_tz":-540,"elapsed":596,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","# Word2Vec 모델 생성\n","wv_model = Word2Vec(train_sentence,        #학습할 문장\n","                    vector_size=SIZE,             #워드 임베딩 벡터의 크기\n","                    window=WINDOW,         #문맥 윈도우 크기\n","                    max_vocab_size=500,    #학습에 사용될 최대 단어 수\n","                    min_count=MIN_COUNT,   #학습에 사용할 최소 단어 빈도 수\n","                    workers=4,              #학습을 위한 프로세스 수\n","                    epochs=10,               #학습 횟수\n","                    sg=1)                  #skip-Gram방식사용"],"metadata":{"id":"s42Yr9cbJ3uf","executionInfo":{"status":"ok","timestamp":1683345856012,"user_tz":-540,"elapsed":631,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["* Word2Vec 모델로부터 데이터를 벡터화하기 위한 함수 생성"],"metadata":{"id":"PVSSgw6jz-RH"}},{"cell_type":"code","source":["# Word2Vec 모델로부터 하나의 문장을 벡터화 시키는 함수\n","def get_sent_embedding(model, embedding_size, tokenized_words):\n","    # 임베딩 벡터를 0으로 초기화\n","    feature_vec = np.zeros((embedding_size,), dtype='float32')\n","    # 단어 개수 초기화\n","    n_words = 0\n","    # 모델 단어 집합 생성\n","    index2word_set = set(model.wv.index2word)\n","    # 문장의 단어들을 하나씩 반복\n","    for word in tokenized_words:\n","        # 모델 단어 집합에 해당하는 단어일 경우에만\n","        if word in index2word_set:\n","            # 단어 개수 1 증가\n","            n_words += 1\n","            # 임베딩 벡터에 해당 단어의 벡터를 더함\n","            feature_vec = np.add(feature_vec, model[word])\n","    # 단어 개수가 0보다 큰 경우 벡터를 단어 개수로 나눠줌 (평균 임베딩 벡터 계산)\n","    if (n_words > 0):\n","        feature_vec = np.divide(feature_vec, n_words)\n","    return feature_vec"],"metadata":{"id":"vGKxMURH0L0R","executionInfo":{"status":"ok","timestamp":1683345858535,"user_tz":-540,"elapsed":495,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# 문장벡터 데이터 셋 만들기\n","def get_dataset(sentences, model, num_features):\n","    dataset = list()\n","\n","    # 각 문장을 벡터화해서 리스트에 저장\n","    for sent in sentences:\n","        dataset.append(get_sent_embedding(model, num_features, sent))\n","\n","    # 리스트를 numpy 배열로 변환하여 반환\n","    sent_embedding_vectors = np.stack(dataset)\n","    \n","    return sent_embedding_vectors"],"metadata":{"id":"sZS9j5lgKDGQ","executionInfo":{"status":"ok","timestamp":1683345861152,"user_tz":-540,"elapsed":468,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["* 이제 학습데이터의 Q를 Word2Vec 모델을 사용하여 벡터화 합니다."],"metadata":{"id":"DEjSR247a9iw"}},{"cell_type":"code","source":["# 학습 데이터의 문장들을 Word2Vec 모델을 사용하여 벡터화\n","train_data_vecs = get_dataset(train_sentence, wv_model, SIZE)\n","train_data_vecs.shape\n"],"metadata":{"id":"bIoehvZna82t","colab":{"base_uri":"https://localhost:8080/","height":372},"executionInfo":{"status":"error","timestamp":1683345863979,"user_tz":-540,"elapsed":842,"user":{"displayName":"이신비","userId":"04527371589362270556"}},"outputId":"3ad4f565-8ece-4577-cdbc-a7d135eb910c"},"execution_count":15,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-7275e5c0b2d9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 학습 데이터의 문장들을 Word2Vec 모델을 사용하여 벡터화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_data_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-2acb8bfde04b>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(sentences, model, num_features)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 각 문장을 벡터화해서 리스트에 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sent_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 리스트를 numpy 배열로 변환하여 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-023bbf079c32>\u001b[0m in \u001b[0;36mget_sent_embedding\u001b[0;34m(model, embedding_size, tokenized_words)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# 모델 단어 집합 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mindex2word_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 문장의 단어들을 하나씩 반복\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mindex2word\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    724\u001b[0m             \u001b[0;34m\"The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0;34m\"See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"]}]},{"cell_type":"markdown","source":["* 훈련된 Word2Vec 모델을 사용하여 문장 목록에 대한 문장 임베딩을 생성하고 이를 2차원 numpy 배열에 저장합니다. \n","* 그런 다음 이러한 임베딩을 다양한 기계 학습 모델의 입력 기능으로 사용할 수 있습니다"],"metadata":{"id":"l9XFPz28RmlE"}},{"cell_type":"markdown","source":["### (2) 분류 모델링\n","* 데이터 분할\n","    * x, y\n","        * x : 이전 단계에서 저장된 임베딩벡터(train_data_vecs)\n","        * y : intent 값들\n","    * train, val\n","        * train_test_split 활용\n","* 머신러닝 모델링\n","    * lightGBM, RandomForest 등을 활용하여 학습\n","    * 필요하다면 hyper parameter 튜닝을 시도해도 좋습니다.\n","* validation set으로 검증해 봅시다."],"metadata":{"id":"NOo2RwzWRr0c"}},{"cell_type":"code","source":["# X와 y 데이터 분리\n","x = train_data_vecs\n","y = np.array(list(train['intent']))\n","# Train-Test split\n","x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=1)\n","x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, stratify=y, random_state=1)"],"metadata":{"id":"vvS1F0EoKTv-","executionInfo":{"status":"aborted","timestamp":1683345334330,"user_tz":-540,"elapsed":14,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 모델1"],"metadata":{"id":"qKuYKmQ2dbP1"}},{"cell_type":"code","source":["lgbm = LGBMClassifier(n_estimators=200, objective='multiclass')\n","\n","lgbm.fit(x_train, y_train)"],"metadata":{"id":"MXDMP5gUbQsr","executionInfo":{"status":"aborted","timestamp":1683345334331,"user_tz":-540,"elapsed":15,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred1 = lgbm.predict(x_val)\n","accuracy_score(y_val, pred1)"],"metadata":{"id":"CPPQ3lejdWwq","executionInfo":{"status":"aborted","timestamp":1683345334332,"user_tz":-540,"elapsed":15,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 모델2"],"metadata":{"id":"uq63HIFjdc8c"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_estimators=200)\n","\n","rf.fit(x_train, y_train)"],"metadata":{"id":"4GQPYXMncvkh","executionInfo":{"status":"aborted","timestamp":1683345334333,"user_tz":-540,"elapsed":16,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred2 = rf.predict(x_val)\n","accuracy_score(y_val, pred2)"],"metadata":{"id":"PEXD7iWmDEem","executionInfo":{"status":"aborted","timestamp":1683345334335,"user_tz":-540,"elapsed":18,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델저장\n"],"metadata":{"id":"5LABVwhO6S78"}},{"cell_type":"code","source":["import joblib\n","\n","joblib.dump(lgbm, path+'lgbm_classifier.pkl')"],"metadata":{"id":"0OTZc4dP6Wxn","executionInfo":{"status":"aborted","timestamp":1683345334336,"user_tz":-540,"elapsed":19,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축"],"metadata":{"id":"NxeL02Q1TecY"}},{"cell_type":"markdown","source":["* **상세요구사항**\n","    * 챗봇 flow : input 질문 -> 분류 모델로 intent 예측 --> intent에 해당하는 답변 출력\n","        * 하나의 intent 에는 여러 답변이 있습니다. 이중 한가지를 랜덤하게 선택합니다."],"metadata":{"id":"thqkcWLsTiwc"}},{"cell_type":"markdown","source":["#### 1) 데이터 중 하나에 대해서 테스트"],"metadata":{"id":"GLHnFhw-3dWl"}},{"cell_type":"code","source":["n = 1\n","test['Q'].loc[n]"],"metadata":{"id":"8HcLTNykBQHh","executionInfo":{"status":"aborted","timestamp":1683345334337,"user_tz":-540,"elapsed":19,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#입력문장 벡터화\n","test_data_vec = get_dataset([test['Q'].iloc[n]], wv_model, SIZE)"],"metadata":{"id":"A55XDY5VBQE5","executionInfo":{"status":"aborted","timestamp":1683345334337,"user_tz":-540,"elapsed":19,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#분류 모델을 이용하여 intent 예측\n","p1 = lgbm.predict(test_data_vec)[0]\n","p2 = rf.predict(test_data_vec)[0]\n","p1, p2"],"metadata":{"id":"8R-7o3Q9BQB3","executionInfo":{"status":"aborted","timestamp":1683345334338,"user_tz":-540,"elapsed":20,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('질문: ', test['Q'].iloc[n])\n","answer1 = train.loc[train['intent']==p1, 'A'].unique()\n","print('답변1: ', np.random.choice(answer1))\n","\n","answer2 = train.loc[train['intent']==p2, 'A'].unique()\n","print('답변2: ', np.random.choice(answer2))"],"metadata":{"id":"901OF6AvBP-u","executionInfo":{"status":"aborted","timestamp":1683345334339,"user_tz":-540,"elapsed":21,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Yar1bTRTBP7N","executionInfo":{"status":"aborted","timestamp":1683345334339,"user_tz":-540,"elapsed":20,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 챗봇 함수 만들기\n","* 테스트 코드를 바탕으로 질문을 받아 답변을 하는 함수를 생성합시다.\n","* 성능이 좋은 모델 사용. "],"metadata":{"id":"ZuF2udm93j7W"}},{"cell_type":"code","source":["def get_answer1(question): \n","    test_data_vec = get_dataset([question], wv_model, SIZE)\n","    pred = rf.predict(test_data_vec)[0]\n","    answers = train.loc[train['intent']==pred, 'A'].unique()\n","    answer = np.random.choice(answers)\n","\n","    return answer, pred"],"metadata":{"id":"o6YSzGDe3n0M","executionInfo":{"status":"aborted","timestamp":1683345334340,"user_tz":-540,"elapsed":21,"user":{"displayName":"이신비","userId":"04527371589362270556"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = '4년제 졸업자만 지원 가능한가요?'\n","get_answer1(question)"],"metadata":{"id":"oUi-QgPD4HnP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681780646362,"user_tz":-540,"elapsed":387,"user":{"displayName":"이신비","userId":"04527371589362270556"}},"outputId":"f01a978f-5ff9-4f93-f3fb-6961bc9effbd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('학교 사정으로 졸업예정증명서 발급이 불가능한 경우 수료증명서 또는 재학증명서를 제출하여 주시기 바라며, 추후 졸업예정증명서 발급 가능 시점에 제출 요청을 드릴 수 있습니다.\\n대학원 재학 중으로 성적증명서가 없으신 경우, 최종학력은 대학교로 입력하여 주시고 졸업(예정)증명서 및 성적증명서 제출란에는 재학증명서를 제출하여 주시기 바랍니다.\\n국외 대학의 경우, 졸업을 확인할 수 있는 서류인 공식 졸업장(예: Diploma)과, 4년간의 이수 과목과 성적을 확인할 수 있는 성적표(예: Transcript)를 제출하시면 됩니다.',\n"," 39)"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"V1a2FMV5yqwi"}},{"cell_type":"code","source":["ct = 0\n","for tdx, row in test.iterrows():\n","    print('질문: ', row['Q'])\n","    answer, intent_idx = get_answer1(row['Q'])\n","    print('예상 답변: ', answer)\n","    print('-'*200) \n","    if intent_idx == row['intent']:\n","        ct += 1\n","print(ct, test.shape[0])"],"metadata":{"id":"B8nXCSHX92HI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RWQ_Tv9tBdyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.챗봇2\n","\n","* **세부요구사항**\n","    * 단계별 챗봇을 만들어 봅시다.\n","        * 1단계 : type을 0과 1로 분류하는 모델 생성(Embedding + LSTM 모델)\n","        * 2단계 : \n","            * 각 type에 맞게, 사전학습된 Word2Vec 모델을 사용하여 임베딩 벡터(train)를 만들고\n","        * 3단계 : 챗봇 만들기\n","            * input 문장과 train 임베딩 벡터와 코사인 유사도 계산\n","            * 가장 유사도가 높은 질문의 intent 찾아\n","            * 해당 intent의 답변 중 무작위로 하나를 선정하여 답변하기"],"metadata":{"id":"6FAB06MnP5qf"}},{"cell_type":"markdown","source":["### (1) 1단계 : type 분류 모델링(LSTM)\n","- LSTM"],"metadata":{"id":"XvAzrVuvVQT9"}},{"cell_type":"markdown","source":["#### 1) 데이터 준비\n","* 학습용 데이터를 만들어 봅시다.\n","    * 시작 데이터 : clean_train_questions, clean_test_questions\n","    * 각 토큰에 인덱스를 부여하는 토크나이저를 만들고 적용\n","        * from tensorflow.keras.preprocessing.text import Tokenizer 를 사용\n","    * 문장별 길이에 대한 분포를 확인하고 적절하게 정의."],"metadata":{"id":"VklTJM3-tuDQ"}},{"cell_type":"code","source":["# 각각의 토큰에 인덱스 부여하는 토크나이저 선언\n","tokenizer = Tokenizer()\n","\n","# .fit_on_tests 이용하여 토크나이저 만들기\n","tokenizer.fit_on_textx(clean_train_questions)"],"metadata":{"id":"TbpMdJT3t228"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 토큰의 수 확인\n","import json\n","print(json.loads(tokenizer.to_json())['config']['word_index'])\n","len(tokenizer.word_index.keys())"],"metadata":{"id":"ft3qCoGbuIt4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 토큰의 수로 vocab_size 지정\n","vocab_size = len(tokenizer.word_index.keys())\n","tokenizer = Tokenizer(vocab_size, oov_token = '<ooV>')\n","\n","# fit_on_texts을 위에서 한번만 해도 되지만, vocab 사이즈를 확인하고 줄이거나 하는 시도를 할 수도 있기에 다시 수행\n","tokenizer.fit_on_texts(clean_train_questions)\n","\n","# .texts_to_sequences : 토크나이즈 된 데이터를 가지고 모두 시퀀스로 변환\n","x_train = tokenizer.texts_to_sequences(clean_train_questions)\n","x_val = tokenizer.texts_to_sequences(clean_test_qeustions)"],"metadata":{"id":"mOpw0-EwuJ9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 토큰의 수 확인\n","import json\n","print(json.loads(tokenizer.to_json())['config']['word_index'])\n","len(tokenizer.word_index.keys())"],"metadata":{"id":"xPzUr5t4Q0q0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 각 토큰과 인덱스로 구성된 딕셔너리 생성,\n","word_vocab = tokenizer.word_index\n","\n","# <PAD> 는 0으로 추가\n","word_vocab['<PAD>'] = 0\n","vocab_size = len(tokenizer.word_index.keys())"],"metadata":{"id":"sVHleK_muYl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 토큰의 수 확인\n","import json\n","print(json.loads(tokenizer.to_json())['config']['word_index'])\n","len(tokenizer.word_index.keys())"],"metadata":{"id":"LUVX9pLjRC9I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 문장별 토큰수에 대해 탐색적 분석을 수행해 봅시다."],"metadata":{"id":"38FwBFRll05a"}},{"cell_type":"code","source":["# 문장별 토큰 수 카운트\n","train_word_counts = clean_train_questions.apply(lambda x: len(x.split(' ')))\n","# 기초 통계량\n","display(pd.DataFrame(train_word_counts).describe().T)\n","# 분포 그래프\n","plt.fijure(figsize=(15, 10))\n","sns.histplot(train_word_counts, bins=50, label='train')\n","plt.legend()\n","plt.grid()\n","plt.xlabel('Number of words', fontsize=15)\n","plt.ylabel('Number of questions', fontsize=15)"],"metadata":{"id":"6hwGM_0dubPR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 문장별 토큰이 가장 큰 것이 57개 입니다. "],"metadata":{"id":"nevMXd0vmj8H"}},{"cell_type":"markdown","source":["* 학습 입력을 위한 데이터 크기 맞추기\n","    * 문장이 짧기 때문에 MAX_SEQUENCE_LENGTH는 정하지 않아도 되지만,\n","    * 그러나 분포를 보고 적절하게 자릅시다.\n","    * 그리고 pad_sequences 함수를 이용하여 시퀀스데이터로 변환하기\n","* y는 train['type'] 와 test['type'] 입니다."],"metadata":{"id":"wQfbaY3HmIMj"}},{"cell_type":"code","source":["MAX_SEQUENCE_LENGTH = 20\n","\n","x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n","x_val = pad_sequences(x_val, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"],"metadata":{"id":"8M2QWrD-ups7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train.shape"],"metadata":{"id":"6nlnE9dju5Dh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train = train['type'].values\n","y_val = test['type'].values\n","y_train.shape, y_val.shape"],"metadata":{"id":"EMlo9o1VngW2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 모델링\n","\n","* 토크나이징 한 데이터를 입력으로 받아 \n","* Embedding 레이어와 LSTM 레이어를 결합하여 이진 분류 모델링을 수행합니다."],"metadata":{"id":"WlBeVPA_6ePq"}},{"cell_type":"code","source":["classifier_model = Sequential([\n","                    Embedding(input_dim=vocab_size, output_dim=64),\n","                    LSTM(32, return_sequences=True),\n","                    LSTM(16),\n","                    Dense(8),\n","                    Dense(4),\n","                    Dense(1, activation='sigmoid')\n","                    ])"],"metadata":{"id":"8ilRqzlKScHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_model.summary()"],"metadata":{"id":"0CaxzPmySjte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)"],"metadata":{"id":"11jhI4eOSlOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","history = classifier_model.fit(x_train, y_train, callbacks=[es], validation_split=0.2, epochs=10, batch_size=32)"],"metadata":{"id":"qBeQBXhzSm0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = classifier_model.predict(x_val)\n","pred = np.where(pred >= 0.5, 1, 0)\n","\n","print(accuracy_score(y_val, pred))"],"metadata":{"id":"KOhx2rzGFmiy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (2) 사전학습모델(Word2Vec)"],"metadata":{"id":"NqpAqObGdgbu"}},{"cell_type":"markdown","source":["* Pre-trained Word2Vec model\n","    * 이미 다운로드 받아서 제공되었습니다.\n","        * ko.bin, ko.tsv\n","    * 참고 사이트: https://github.com/Kyubyong/wordvectors\n","        * 모델 파일 다운로드 사이트: https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view?resourcekey=0-Dq9yyzwZxAqT3J02qvnFwg\n","* 사전학습 모델을 로딩하고, \n","* train 데이터셋의 질문(Q)을 임베딩벡터로 만들어, 열(Column)로 추가합니다."],"metadata":{"id":"eJWm6cKwIZjf"}},{"cell_type":"markdown","source":["#### 1) 모델 로딩\n","* 사전 학습된 모델을 로딩 : gensim.models.Word2Vec.load()\n","* 로딩 후 벡터 크기를 조회합시다. .vector_size"],"metadata":{"id":"a5iIl1X57hef"}},{"cell_type":"code","source":["import gensim\n","pre_wv_model = gensim.models.Word2Vec.load(path + 'ko.bin')"],"metadata":{"id":"Wu2_fSn9Vjxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gdown\n","if os.path.exists(os.path.join(path, 'ko.zip')) is None:\n","    url = 'https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view?resourcekey=0-Dq9yyzwZxAqT3J02qvnFwg'\n","    output = os.path.join(path, 'ko.zip')\n","    gdown.download(url, output, fuzzy=True, quiet=False)\n","    import zipfile\n","    with zipfile.ZipFile(os.path.join(path, \"ko.zip\"), 'r') as zip_ref:\n","        zip_ref.extractall(path)"],"metadata":{"id":"FnQIAV0wTg3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델의 벡터크기 조회\n","pre_mv_model.vector_size\n"],"metadata":{"id":"Xmq6Fh5lWN0Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) train 에 임베딩벡터 결과 저장\n","* get_sent_embedding 함수를 이용하여 train의 질문별 임베딩 결과를 저장합니다.\n","    * .apply(lambda .....) 를 활용하세요."],"metadata":{"id":"-reuWNEy7ogd"}},{"cell_type":"code","source":["train['w2v'] = train['Q'].apply(lambda sent: get_sent_embedding(pre_mv_model, pre_mv_model.vector_size, tokenize('kkma', sent, join=False)))"],"metadata":{"id":"I17jau4YWSD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Sq3lQ798WVCF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축\n","* 아래 절차대로 수행하는 함수 만들기\n","    * input 질문 \n","    * 1단계 : 모델을 이용하여 type 0, 1로 분류\n","    * 2단계 : \n","        * train의 모든 Q와 input 문장의 임베딩 벡터간의 코사인 유사도 계산\n","        * 코사인 유사도가 가장 높은 Q를 선택\n","        * 선택한 Q의 intent에 맵핑된 답변 중 하나를 무작위로 선택"],"metadata":{"id":"rilxqf-cX4cN"}},{"cell_type":"markdown","source":["#### 1) 하나의 질문으로 테스트해보기"],"metadata":{"id":"4fUJIN1Sc0YJ"}},{"cell_type":"markdown","source":["* 선택된 질문과 답변"],"metadata":{"id":"8ozGvY19c4xd"}},{"cell_type":"code","source":["test_question = test['Q'].iloc[0]\n","print('질문: ', test_question)\n","print('답변: ', test['A'].iloc[0])\n","print('세부 카테고리: ' , test['intent'].iloc[0])"],"metadata":{"id":"Zo41eD7Vnd6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측을 위한 입력 형태로 변환\n","    * 학습을 위한 전처리 과정을 test 데이터에도 적용합니다. "],"metadata":{"id":"23bZbPscdA0n"}},{"cell_type":"code","source":["tokenized_sent = tokenize('mecab', test_question)\n","tokenized_sent_seq = tokenizer.texts_to_sequences([tokenized_sent])\n","tokenized_sent_input = pad_sequences(tokenized_sent_seq, amxlen=MAX_SEQUENCE_LENGTH)"],"metadata":{"id":"qbrsu2szX6xL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 1단계 : type 분류"],"metadata":{"id":"GZzJEnFKdnPe"}},{"cell_type":"code","source":["pred_type = classifier_model.predict(tokenized_sent_input)\n","pred_type = np.where(pred_type >= 0.5, 1, 0)[0][0]\n","pred_type"],"metadata":{"id":"1U6yjYqTp3Nj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 2단계 : 질문에 대한 벡터 만들고 코사인 유사도 계산\n","    * Word2Vec 사전 학습 모델로 부터 벡터 만들기"],"metadata":{"id":"BTC3Rn9Tdtwo"}},{"cell_type":"code","source":["test_question_w2v = get_sent_embedding(pred_wv_model, pred_wv_model.vector_size, tokenize('kkma', test_question, join=False)))"],"metadata":{"id":"dGpWRXtDqNti"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* train의 질문 벡터들과 유사도 계산\n","    * Word2Vec 으로 만든 벡터들과 유사도 계산"],"metadata":{"id":"JOmz_Ib_d6-R"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","temp_df = train[train['type'] == pred_type].copy()\n","\n","temp_df['cossim_w2v'] = temp_df['w2v'].apply(lambda x: cosine_similarity([test_question]))\n","temp_df.head()\n","\n","\n","\n"],"metadata":{"id":"EQchLoOJZOPN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 챗봇 함수 만들기\n","* 위 테스트 결과를 바탕으로 코드를 정리하고 함수로 생성합니다."],"metadata":{"id":"OPSCaDE5ro5F"}},{"cell_type":"code","source":["def get_answer2(question): \n","    tokenizer_sent = tokenize('mecab', question)\n","\n","    tokenized_sent_seq = tokenizer.texts_to_sequences([tokenized_sent])\n","    tokenized_sent_input = pad_sequences(tokenized_sent_seq, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","    pred_type = classifier_model.predict(tokenized_sent_input)\n","    pred_type = np.where(pred_type >= 0.5, 1, 0)[0][0]\n","    test_question_s2v = get_sent_embedding(pred_wv_model, pred_wv_model.vector_size, )\n","\n","    temp_df = train[train['type'] == pred_type].copy()\n","    temp_df['cossim_w2v'] = temp_df['w2v'].apply(lambda x: cosine_similarity([test_question]))\n","\n","    intent_idx = temp_df.sort_values(['cossim_w2v'], ascending=False).head(1)['intent']\n","    answers = train.loc[train['train'] == intent_idx, 'A'].unique()\n","    answer = np.random.choice(answers)\n","\n","    return answer, intent_idx"],"metadata":{"id":"vK9rO8w6rTRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = '지원 자격이 궁금해요'\n","get_answer2(question)"],"metadata":{"id":"7icv4nEt9aLQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"C8cu77Si9rzu"}},{"cell_type":"code","source":[],"metadata":{"id":"EJL_C-Mmtk9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.챗봇3\n","\n","* **세부요구사항**\n","    * 단계별 챗봇을 만들어 봅시다.\n","        * 1단계 : 챗봇2의 1단계 모델을 사용합니다.\n","        * 2단계 : \n","            * 각 type에 맞게, 사전학습된 Word2Vec 모델을 사용하여 임베딩 벡터(train)를 만들고\n","        * 3단계 : 챗봇 만들기\n","            * input 문장과 train 임베딩 벡터와 코사인 유사도 계산\n","            * 가장 유사도가 높은 질문의 intent 찾아\n","            * 해당 intent의 답변 중 무작위로 하나를 선정하여 답변하기"],"metadata":{"id":"TNn40nu96PI4"}},{"cell_type":"markdown","source":["### (1) 1단계 : type 분류 모델링\n","- LSTM : 3-(1) 모델을 그대로 사용합니다."],"metadata":{"id":"iK7s4ZCt6PI_"}},{"cell_type":"code","source":[],"metadata":{"id":"pOftkHKGAGXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (2) FastText 모델"],"metadata":{"id":"ebdw6-K9AR_V"}},{"cell_type":"markdown","source":["-  FastText 모델 학습을 위한 입력 포맷 2차원 리스트 형태 입니다.\n","  ```\n","  [['나', '는', '학생', '이다'], ['오늘', '은', '날씨', '가', '좋다']]\n","  ```"],"metadata":{"id":"9RQnEBcnAZNt"}},{"cell_type":"markdown","source":["- Word2Vec계열의 FastText를 학습하는 이유\n","  - n-gram이 추가된 fasttext 모델은 유사한 단어에 대한 임베딩을 word2vec보다 잘 해결할 수 있으며, 오탈자 등에 대한 임베딩 처리가 가능하다.\n","  - 예) 체크카드, 쳌카드는 word2vec에서는 전혀 다른 단어이지만 fasttext는 character n-gram으로 비교적 같은 단어로 처리할 수 있다.\n","- 참고: https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText\n"],"metadata":{"id":"ghuzogkoAZNt"}},{"cell_type":"markdown","source":["#### 1) 데이터 준비\n","* 시작데이터 : clean_train_questions, clean_test_questions"],"metadata":{"id":"HTGtH4bPAh-8"}},{"cell_type":"markdown","source":["* FastText를 위한 입력 데이터 구조 만들기"],"metadata":{"id":"cQIJ4IPZ0b9C"}},{"cell_type":"code","source":["fasttext_train_sentences = [x.split(' ') for x in clean_train_questions.values]"],"metadata":{"id":"Y-kEQiETAZNt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) FastText 모델 생성\n","* FastText 문법\n","    * FastText( input데이터,  min_count = , size= , window=  )\n","        * input데이터 : 학습에 사용할 문장으로 이루어진 리스트\n","        * min_count : 모델에 사용할 단어의 최소 빈도수. 이 값보다 적게 출현한 단어는 모델에 포함되지 않음. 기본값 = 5\n","        * size : 단어의 벡터 차원 지정. 기본값 = 100\n","        * window : 학습할 때 한 단어의 좌우 몇 개의 단어를 보고 예측을 할 것인지를 지정. 기본값 = 5\n","    * 참조 : https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"],"metadata":{"id":"46o8NG_LAk7c"}},{"cell_type":"code","source":["from gensim.models.fasttext import FastText\n","import gensim.models.word2vec\n","ft_model = FastText(fasttext_train_sentences, min_count=2, size=200, window=3)\n","print(ft_model)"],"metadata":{"id":"xlWLTr2nAZNu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) train에 임베딩벡터 결과 저장\n","* get_sent_embedding 함수를 이용하여 train의 질문별 임베딩 결과를 저장합니다.\n","    * .apply(lambda .....) 를 활용하세요."],"metadata":{"id":"NilgkdYPAo0s"}},{"cell_type":"code","source":["train['ft_vec'] = train['Q'].apply(lambda sent: get_sent_embedding(ft_model, ft_model.vector_size, tokenize('mecab', sent, join=False)))\n","train.head()"],"metadata":{"id":"K2iZx5hRAZNu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축\n","- input 질문\n","- intent classifier로 common와 faq 중 하나를 예측\n","- 예측된 intent에 속한 train의 모든 Q와 input 문장의 임베딩 벡터간의 코사인 유사도 계산\n","- 코사인 유사도가 가장 높은 top-3개의 Q를 선택\n","- 선택한 Q에 맵핑된 답변 중 하나를 선택하고 실제 답변과 비교"],"metadata":{"id":"HUoQgTf46PJD"}},{"cell_type":"markdown","source":["#### 1) 하나의 질문으로 테스트해보기"],"metadata":{"id":"esu6rcsJ6PJD"}},{"cell_type":"markdown","source":["* 선택된 질문과 답변"],"metadata":{"id":"dCxYTX2W6PJD"}},{"cell_type":"code","source":["test_question = test['Q'].iloc[0]\n","print('질문: ', test_question)\n","print('답변: ', test['A'].iloc[0])\n","print('세부 카테고리: ' , test['intent'].iloc[0])"],"metadata":{"id":"9Nl6O3Di6PJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측을 위한 입력 형태로 변환"],"metadata":{"id":"g-8Ck-Mh6PJD"}},{"cell_type":"code","source":["tokenized_sent = tokenize('mecab', test_question)\n","tokenized_sent_seq = tokenizer.texts_to_sequences([tokenized_sent])\n","tokenized_sent_input = pad_sequences(tokenized_sent_seq, amxlen=MAX_SEQUENCE_LENGTH)"],"metadata":{"id":"j0yGO9yQ6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측하기"],"metadata":{"id":"-yp2MBmo6PJE"}},{"cell_type":"code","source":["pred_type = classifier_model.predict(tokenized_sent_input)\n","pred_type = np.where(pred_type >= 0.5, 1, 0)[0][0]\n","pred_type"],"metadata":{"id":"BYm1AODK6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 질문에 대한 벡터 만들기\n","    * FestText 모델로 부터 벡터 만들기"],"metadata":{"id":"uF0D3SH16PJE"}},{"cell_type":"code","source":["test_question_ft = get_sent_embedding(ft_model, ft_model.vector_size, tokenize('kkma', test_question, join=False))\n"],"metadata":{"id":"i_6k-Tjf6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* train의 질문 벡터들과 유사도 계산\n","    * FastText 로 만들 벡터들과 유사도 계산"],"metadata":{"id":"rgEVj1WG6PJE"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","temp_df = train[train['type'] == pred_type].copy()\n","\n","temp_df['cossim_ft'] = temp_df['ft_vec'].apply(lambda x: cosine_similarity([test_question]))\n","temp_df.head()\n","\n"],"metadata":{"id":"OSX-j8WV6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### 2) 함수로 생성하기"],"metadata":{"id":"q-IJcMH26PJF"}},{"cell_type":"code","source":["def get_answer3(question): \n","    tokenizer_sent = tokenize('mecab', question)\n","\n","    tokenized_sent_seq = tokenizer.texts_to_sequences([tokenized_sent])\n","    tokenized_sent_input = pad_sequences(tokenized_sent_seq, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","    pred_type = classifier_model.predict(tokenized_sent_input)\n","    pred_type = np.where(pred_type >= 0.5, 1, 0)[0][0]\n","    test_question_s2v = get_sent_embedding(pred_wv_model, pred_wv_model.vector_size, )\n","\n","    temp_df = train[train['type'] == pred_type].copy()\n","    temp_df['cossim_ft'] = temp_df['ft_vec'].apply(lambda x: cosine_similarity([test_question]))\n","\n","    intent_idx = temp_df.sort_values(['cossim_ft'], ascending=False).head(3).index\n","    choiced-idx = np.random.choice(intent_idx)\n","    answer = train.loc[choiced_idx, 'A']\n","\n","    return answer, train.loc[choiced_idx, 'intent']"],"metadata":{"id":"dYlaEeNN6PJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = '나이 제한이 있나요'\n","get_answer3(question)"],"metadata":{"id":"gONYObLpBwgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_model.wv.similar_by_word['나이']"],"metadata":{"id":"gyYSXO-obM2_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"GNfo5m5Hyswe"}},{"cell_type":"code","source":["ct = 0\n","for idx, row in test.iterrows():\n","    print('질문: ', row['Q'])\n","    answer, intext_idx = get_answer3(row['Q'])\n","    print('예상 답변: ', answer)\n","    if intent_idx == row['intent']:\n","        ct += 1\n","print(ct, test.shape[0])"],"metadata":{"id":"IKFBp5LMBwgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fF1JMFRs6PJG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.질문에 대한 답변 비교해보기\n","\n","* **세부요구사항**\n","    * 세가지 챗봇을 생성해 보았습니다. \n","    * 질문을 입력하여 답변을 비교해 봅시다. 어떤 챗봇이 좀 더 정확한 답변을 하나요?\n"],"metadata":{"id":"BEjQHVAJCKkN"}},{"cell_type":"code","source":[],"metadata":{"id":"c8q4Vu5CCvSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-i0609ZGDYYd"},"execution_count":null,"outputs":[]}]}